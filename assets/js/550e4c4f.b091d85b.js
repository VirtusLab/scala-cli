"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2293],{9589:function(e,a,n){n.r(a),n.d(a,{contentTitle:function(){return l},default:function(){return m},frontMatter:function(){return s},metadata:function(){return u},toc:function(){return c}});var t=n(3117),r=n(102),o=(n(7294),n(3905)),p=n(9705),i=["components"],s={},l="Experimental Spark features",u={type:"mdx",permalink:"/spark",source:"@site/src/pages/spark.md",title:"Experimental Spark features",description:"Packaging",frontMatter:{}},c=[{value:"Packaging",id:"packaging",level:2},{value:"Running Spark jobs",id:"running-spark-jobs",level:2},{value:"Running Spark jobs in a standalone way",id:"running-spark-jobs-in-a-standalone-way",level:2},{value:"Running Hadoop jobs",id:"running-hadoop-jobs",level:2}],d={toc:c};function m(e){var a=e.components,n=(0,r.Z)(e,i);return(0,o.kt)("wrapper",(0,t.Z)({},d,n,{components:a,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"experimental-spark-features"},"Experimental Spark features"),(0,o.kt)("h2",{id:"packaging"},"Packaging"),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"package")," sub-command offers to package Scala CLI projects as JARs ready to be passed\nto ",(0,o.kt)("inlineCode",{parentName:"p"},"spark-submit"),", and optimized for it."),(0,o.kt)(p.v,{mdxType:"ChainedSnippets"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala",metastring:"title=SparkJob.scala",title:"SparkJob.scala"},'//> using lib "org.apache.spark::spark-sql:3.0.3"\n//> using scala "2.12.15"\n\nimport org.apache.spark._\nimport org.apache.spark.sql._\n\nobject SparkJob {\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession.builder()\n      .appName("Test job")\n      .getOrCreate()\n    import spark.implicits._\n    def sc    = spark.sparkContext\n    val accum = sc.longAccumulator\n    sc.parallelize(1 to 10).foreach(x => accum.add(x))\n    println("Result: " + accum.value)\n  }\n}\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"scala-cli package --spark SparkJob.scala -o spark-job.jar\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-text"},"Compiling project (Scala 2.12.15, JVM)\nCompiled project (Scala 2.12.15, JVM)\nWrote spark-job.jar\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"spark-submit spark-job.jar\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-text"},"\u2026\nResult: 55\n\u2026\n"))),(0,o.kt)("h2",{id:"running-spark-jobs"},"Running Spark jobs"),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"run")," sub-command can run Spark jobs, when passed ",(0,o.kt)("inlineCode",{parentName:"p"},"--spark"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"scala-cli run --spark SparkJob.scala # same example as above\n")),(0,o.kt)("p",null,"Note that this requires either"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"spark-submit")," to be in available in ",(0,o.kt)("inlineCode",{parentName:"li"},"PATH")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"SPARK_HOME")," to be set in the environment")),(0,o.kt)("h2",{id:"running-spark-jobs-in-a-standalone-way"},"Running Spark jobs in a standalone way"),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"run")," sub-command can not only run Spark jobs, but it can also work without a Spark\ndistribution. For that to work, it downloads Spark JARs, and calls the main class of\n",(0,o.kt)("inlineCode",{parentName:"p"},"spark-submit")," itself via these JARs:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"scala-cli run --spark-standalone SparkJob.scala # same example as above\n")),(0,o.kt)("h2",{id:"running-hadoop-jobs"},"Running Hadoop jobs"),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"run")," sub-command can run Hadoop jobs, by calling the ",(0,o.kt)("inlineCode",{parentName:"p"},"hadoop jar")," command under-the-hood:"),(0,o.kt)(p.v,{mdxType:"ChainedSnippets"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-java",metastring:"title=WordCount.java",title:"WordCount.java"},'//> using lib "org.apache.hadoop:hadoop-client-api:3.3.3"\n\n// from https://hadoop.apache.org/docs/r3.3.3/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCount {\n\n  public static class TokenizerMapper\n       extends Mapper<Object, Text, Text, IntWritable>{\n\n    private final static IntWritable one = new IntWritable(1);\n    private Text word = new Text();\n\n    public void map(Object key, Text value, Context context\n                    ) throws IOException, InterruptedException {\n      StringTokenizer itr = new StringTokenizer(value.toString());\n      while (itr.hasMoreTokens()) {\n        word.set(itr.nextToken());\n        context.write(word, one);\n      }\n    }\n  }\n\n  public static class IntSumReducer\n       extends Reducer<Text,IntWritable,Text,IntWritable> {\n    private IntWritable result = new IntWritable();\n\n    public void reduce(Text key, Iterable<IntWritable> values,\n                       Context context\n                       ) throws IOException, InterruptedException {\n      int sum = 0;\n      for (IntWritable val : values) {\n        sum += val.get();\n      }\n      result.set(sum);\n      context.write(key, result);\n    }\n  }\n\n  public static void main(String[] args) throws Exception {\n    Configuration conf = new Configuration();\n    Job job = Job.getInstance(conf, "word count");\n    job.setJarByClass(WordCount.class);\n    job.setMapperClass(TokenizerMapper.class);\n    job.setCombinerClass(IntSumReducer.class);\n    job.setReducerClass(IntSumReducer.class);\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(IntWritable.class);\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n  }\n}\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"scala-cli run --hadoop WordCount.java\n"))))}m.isMDXComponent=!0},9705:function(e,a,n){n.d(a,{m:function(){return p},v:function(){return o}});var t=n(7294),r=n(2004);function o(e){var a=e.children;return t.createElement("div",{className:"runnable-command"},a)}function p(e){var a=e.url;return t.createElement(r.Z,{playing:!0,loop:!0,muted:!0,controls:!0,width:"100%",height:"",url:a})}}}]);