"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[8061],{1151:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>p});const r=JSON.parse('{"type":"mdx","permalink":"/spark","source":"@site/src/pages/spark.md","title":"Experimental Spark features","description":"Packaging","frontMatter":{},"unlisted":false}');var s=a(4848),t=a(8453),o=a(2267);const i={},c="Experimental Spark features",l={},p=[{value:"Packaging",id:"packaging",level:2},{value:"Running Spark jobs",id:"running-spark-jobs",level:2},{value:"Running Spark jobs in a standalone way",id:"running-spark-jobs-in-a-standalone-way",level:2},{value:"Running Hadoop jobs",id:"running-hadoop-jobs",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"experimental-spark-features",children:"Experimental Spark features"})}),"\n","\n",(0,s.jsx)(n.h2,{id:"packaging",children:"Packaging"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"package"})," sub-command offers to package Scala CLI projects as JARs ready to be passed\nto ",(0,s.jsx)(n.code,{children:"spark-submit"}),", and optimized for it."]}),"\n",(0,s.jsxs)(o.Z,{children:[(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-scala",metastring:"title=SparkJob.scala",children:'//> using dep org.apache.spark::spark-sql:3.0.3\n//> using scala 2.12.15\n\nimport org.apache.spark._\nimport org.apache.spark.sql._\n\nobject SparkJob {\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession.builder()\n      .appName("Test job")\n      .getOrCreate()\n    import spark.implicits._\n    def sc    = spark.sparkContext\n    val accum = sc.longAccumulator\n    sc.parallelize(1 to 10).foreach(x => accum.add(x))\n    println("Result: " + accum.value)\n  }\n}\n'})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"scala-cli --power package --spark SparkJob.scala -o spark-job.jar\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"Compiling project (Scala 2.12.15, JVM)\nCompiled project (Scala 2.12.15, JVM)\nWrote spark-job.jar\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"spark-submit spark-job.jar\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\u2026\nResult: 55\n\u2026\n"})})]}),"\n",(0,s.jsx)(n.h2,{id:"running-spark-jobs",children:"Running Spark jobs"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"run"})," sub-command can run Spark jobs, when passed ",(0,s.jsx)(n.code,{children:"--spark"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"scala-cli run --spark SparkJob.scala # same example as above\n"})}),"\n",(0,s.jsx)(n.p,{children:"Note that this requires either"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"spark-submit"})," to be in available in ",(0,s.jsx)(n.code,{children:"PATH"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"SPARK_HOME"})," to be set in the environment"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"running-spark-jobs-in-a-standalone-way",children:"Running Spark jobs in a standalone way"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"run"})," sub-command can not only run Spark jobs, but it can also work without a Spark\ndistribution. For that to work, it downloads Spark JARs, and calls the main class of\n",(0,s.jsx)(n.code,{children:"spark-submit"})," itself via these JARs:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"scala-cli run --spark-standalone SparkJob.scala # same example as above\n"})}),"\n",(0,s.jsx)(n.h2,{id:"running-hadoop-jobs",children:"Running Hadoop jobs"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"run"})," sub-command can run Hadoop jobs, by calling the ",(0,s.jsx)(n.code,{children:"hadoop jar"})," command under-the-hood:"]}),"\n",(0,s.jsxs)(o.Z,{children:[(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",metastring:"title=WordCount.java",children:'//> using dep org.apache.hadoop:hadoop-client-api:3.3.3\n\n// from https://hadoop.apache.org/docs/r3.3.3/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCount {\n\n  public static class TokenizerMapper\n       extends Mapper<Object, Text, Text, IntWritable>{\n\n    private final static IntWritable one = new IntWritable(1);\n    private Text word = new Text();\n\n    public void map(Object key, Text value, Context context\n                    ) throws IOException, InterruptedException {\n      StringTokenizer itr = new StringTokenizer(value.toString());\n      while (itr.hasMoreTokens()) {\n        word.set(itr.nextToken());\n        context.write(word, one);\n      }\n    }\n  }\n\n  public static class IntSumReducer\n       extends Reducer<Text,IntWritable,Text,IntWritable> {\n    private IntWritable result = new IntWritable();\n\n    public void reduce(Text key, Iterable<IntWritable> values,\n                       Context context\n                       ) throws IOException, InterruptedException {\n      int sum = 0;\n      for (IntWritable val : values) {\n        sum += val.get();\n      }\n      result.set(sum);\n      context.write(key, result);\n    }\n  }\n\n  public static void main(String[] args) throws Exception {\n    Configuration conf = new Configuration();\n    Job job = Job.getInstance(conf, "word count");\n    job.setJarByClass(WordCount.class);\n    job.setMapperClass(TokenizerMapper.class);\n    job.setCombinerClass(IntSumReducer.class);\n    job.setReducerClass(IntSumReducer.class);\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(IntWritable.class);\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n  }\n}\n'})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"scala-cli run --hadoop WordCount.java\n"})})]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},2267:(e,n,a)=>{a.d(n,{Z:()=>o,b:()=>i});a(6540);var r=a(3554),s=a.n(r),t=a(4848);function o({children:e}){return(0,t.jsx)("div",{className:"runnable-command",children:e})}function i({url:e}){return(0,t.jsx)(s(),{playing:!0,loop:!0,muted:!0,controls:!0,width:"100%",height:"",url:e})}}}]);